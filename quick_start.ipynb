{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.308937  [    0/60000]\n",
      "loss: 2.299855  [ 6400/60000]\n",
      "loss: 2.280784  [12800/60000]\n",
      "loss: 2.273147  [19200/60000]\n",
      "loss: 2.266777  [25600/60000]\n",
      "loss: 2.228288  [32000/60000]\n",
      "loss: 2.241891  [38400/60000]\n",
      "loss: 2.211389  [44800/60000]\n",
      "loss: 2.200930  [51200/60000]\n",
      "loss: 2.164481  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 32.0%, Avg loss: 2.166163 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.177612  [    0/60000]\n",
      "loss: 2.172463  [ 6400/60000]\n",
      "loss: 2.116822  [12800/60000]\n",
      "loss: 2.131139  [19200/60000]\n",
      "loss: 2.094444  [25600/60000]\n",
      "loss: 2.028850  [32000/60000]\n",
      "loss: 2.064559  [38400/60000]\n",
      "loss: 1.989377  [44800/60000]\n",
      "loss: 1.981007  [51200/60000]\n",
      "loss: 1.916044  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.8%, Avg loss: 1.914682 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.942742  [    0/60000]\n",
      "loss: 1.920837  [ 6400/60000]\n",
      "loss: 1.804549  [12800/60000]\n",
      "loss: 1.846933  [19200/60000]\n",
      "loss: 1.744292  [25600/60000]\n",
      "loss: 1.692108  [32000/60000]\n",
      "loss: 1.717080  [38400/60000]\n",
      "loss: 1.615436  [44800/60000]\n",
      "loss: 1.627033  [51200/60000]\n",
      "loss: 1.530326  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.543356 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.601914  [    0/60000]\n",
      "loss: 1.573383  [ 6400/60000]\n",
      "loss: 1.421332  [12800/60000]\n",
      "loss: 1.501048  [19200/60000]\n",
      "loss: 1.377678  [25600/60000]\n",
      "loss: 1.375621  [32000/60000]\n",
      "loss: 1.384546  [38400/60000]\n",
      "loss: 1.309379  [44800/60000]\n",
      "loss: 1.340530  [51200/60000]\n",
      "loss: 1.241899  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 1.268133 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.341196  [    0/60000]\n",
      "loss: 1.327070  [ 6400/60000]\n",
      "loss: 1.161581  [12800/60000]\n",
      "loss: 1.274462  [19200/60000]\n",
      "loss: 1.145254  [25600/60000]\n",
      "loss: 1.172997  [32000/60000]\n",
      "loss: 1.184621  [38400/60000]\n",
      "loss: 1.125432  [44800/60000]\n",
      "loss: 1.164045  [51200/60000]\n",
      "loss: 1.078913  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.099730 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[-0.0173,  0.0169, -0.0286,  ..., -0.0314, -0.0046, -0.0226],\n",
       "                      [-0.0259,  0.0158, -0.0344,  ..., -0.0040, -0.0031,  0.0091],\n",
       "                      [ 0.0084, -0.0067, -0.0314,  ..., -0.0091, -0.0311,  0.0072],\n",
       "                      ...,\n",
       "                      [-0.0132,  0.0079,  0.0063,  ...,  0.0341, -0.0315,  0.0323],\n",
       "                      [-0.0018, -0.0228,  0.0057,  ...,  0.0158, -0.0108, -0.0302],\n",
       "                      [-0.0005, -0.0255, -0.0326,  ...,  0.0038, -0.0043,  0.0079]])),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([ 3.1702e-02, -2.1351e-02,  1.2803e-02,  3.1583e-02, -2.5227e-03,\n",
       "                       2.9534e-02,  1.4413e-02,  1.6015e-04, -3.3959e-02,  1.4816e-02,\n",
       "                      -2.6293e-03,  1.7806e-02,  4.2919e-02,  1.7080e-02, -2.1047e-02,\n",
       "                      -1.4606e-02, -2.6954e-03, -4.0377e-03,  3.1872e-02, -2.9564e-02,\n",
       "                       1.3277e-02, -1.4225e-02,  1.8895e-02,  3.5757e-02,  2.1667e-02,\n",
       "                       3.5089e-02,  3.1818e-02,  9.8045e-03, -2.7361e-02, -1.0774e-02,\n",
       "                      -1.4274e-02, -2.7690e-02,  5.9495e-03, -3.2363e-02,  3.4487e-02,\n",
       "                      -3.3665e-02, -8.0204e-03,  3.7256e-03, -3.0877e-02, -9.2748e-03,\n",
       "                       2.6650e-02, -2.6227e-02,  2.1575e-02,  2.0756e-02,  5.5805e-03,\n",
       "                       1.5055e-02,  2.0426e-02, -1.9428e-02,  1.8623e-02,  4.3570e-03,\n",
       "                       6.6350e-03, -2.2696e-02, -1.5259e-02,  2.8979e-02, -1.2824e-02,\n",
       "                       1.9337e-02, -2.1031e-02, -3.1626e-02, -2.6708e-02, -3.1507e-02,\n",
       "                      -1.9121e-02, -2.8962e-02, -1.2286e-02,  4.2942e-04, -9.5156e-03,\n",
       "                      -2.4699e-02,  7.6540e-04,  3.2354e-02,  1.1034e-02, -3.1475e-02,\n",
       "                       1.7278e-02, -9.9744e-03, -2.3331e-02,  1.3723e-02, -3.4501e-02,\n",
       "                       4.0735e-02,  1.7746e-02,  3.7361e-02,  7.5374e-03, -2.8154e-02,\n",
       "                      -3.7215e-03,  2.0356e-02, -1.2394e-02, -3.1244e-02,  6.0872e-03,\n",
       "                       9.3357e-03,  2.2271e-02,  8.1098e-03, -2.3940e-02,  3.3270e-02,\n",
       "                       3.3729e-03, -1.9946e-02, -3.4315e-03,  2.9298e-02,  5.8622e-03,\n",
       "                       3.8872e-02,  3.1777e-02, -4.4792e-03,  1.7893e-02, -1.9646e-02,\n",
       "                      -2.8578e-02, -1.0853e-02, -1.7307e-02, -4.0477e-03, -2.8017e-05,\n",
       "                       1.8954e-02,  3.7898e-02, -1.0011e-02, -2.1534e-02, -1.5918e-02,\n",
       "                       1.2868e-02,  2.7360e-02, -4.3250e-03,  1.6260e-02,  1.1707e-02,\n",
       "                      -9.6422e-03,  1.4673e-02, -1.5915e-02, -1.7122e-02, -2.6376e-02,\n",
       "                       2.8959e-02,  4.1947e-02, -5.4538e-03,  2.2708e-02,  2.4005e-02,\n",
       "                       2.1287e-02,  6.7869e-03,  2.2580e-02,  2.8013e-02,  1.5253e-02,\n",
       "                       1.0754e-02, -2.3271e-02,  2.4945e-02, -3.5103e-02, -8.5933e-03,\n",
       "                       1.2884e-02, -8.9934e-03,  1.5552e-02, -3.0398e-02,  9.4386e-03,\n",
       "                      -3.1008e-02,  3.2428e-02, -2.3272e-03, -2.2185e-02, -1.4582e-02,\n",
       "                       2.7243e-02,  1.8828e-03,  2.1601e-02,  2.3431e-02, -2.9952e-03,\n",
       "                      -1.9513e-02, -1.7693e-02,  1.2723e-02, -2.7072e-02, -2.9937e-03,\n",
       "                       2.6946e-02, -1.2511e-02, -1.4474e-02,  3.5629e-02,  1.6887e-02,\n",
       "                       3.5907e-03,  1.5830e-02, -4.4628e-03,  1.9747e-02, -1.8184e-02,\n",
       "                       1.0583e-02, -7.5433e-03,  1.1553e-02, -2.5425e-02, -2.4927e-02,\n",
       "                       2.1693e-02, -1.3933e-02, -3.5936e-02, -4.2145e-03,  4.1945e-02,\n",
       "                       1.6337e-02,  2.2081e-02,  1.8705e-03, -2.8785e-02,  3.2879e-02,\n",
       "                      -5.5351e-03,  2.1798e-02,  1.6428e-02,  2.8864e-02,  7.8071e-04,\n",
       "                       1.3800e-02,  3.5530e-02,  1.8658e-02,  8.6191e-03, -8.9733e-04,\n",
       "                       6.7016e-03,  1.1878e-02,  6.0473e-04,  6.3828e-03,  1.8585e-02,\n",
       "                       4.9800e-02,  1.1167e-04, -2.9100e-02,  1.1958e-02,  2.3390e-02,\n",
       "                      -3.6005e-03, -5.1438e-03,  1.3333e-02,  3.0434e-02,  2.5489e-02,\n",
       "                       2.0068e-03,  2.4574e-02, -1.6442e-02,  1.2191e-02,  7.8778e-03,\n",
       "                      -1.8979e-03,  2.3380e-03,  2.0741e-02,  2.8768e-02,  2.7301e-02,\n",
       "                       3.6078e-02, -1.5387e-02, -2.5863e-02,  1.6267e-02, -5.5673e-04,\n",
       "                       1.7764e-02, -3.1583e-02,  2.9701e-02,  6.6012e-03,  1.7937e-02,\n",
       "                       2.3489e-02, -1.5028e-02,  2.9587e-02,  3.6832e-02, -3.1211e-02,\n",
       "                      -1.2688e-03, -2.1722e-02,  3.5241e-02, -5.8606e-03,  1.3339e-02,\n",
       "                       1.6790e-02,  9.8294e-03,  3.1209e-02, -1.6044e-02,  1.7702e-02,\n",
       "                      -4.3743e-03, -2.9728e-02,  1.7423e-02, -1.4540e-02,  6.1750e-03,\n",
       "                       2.8018e-02,  3.3226e-02, -2.9969e-02,  2.6302e-02, -1.0919e-02,\n",
       "                      -2.2981e-02, -1.0788e-02,  4.1089e-02, -1.6025e-02, -6.4833e-03,\n",
       "                       2.8079e-02, -1.6526e-02, -7.2150e-03,  1.0720e-02, -2.1870e-02,\n",
       "                       2.6538e-02, -5.1030e-04,  1.8678e-02,  4.0740e-02, -1.2270e-02,\n",
       "                       1.9639e-02,  1.5160e-02, -2.1084e-02, -2.4169e-03,  3.2976e-02,\n",
       "                      -2.5619e-02,  3.4212e-02, -2.1593e-02,  8.9031e-03, -2.0123e-02,\n",
       "                       3.8306e-02, -1.2564e-03,  3.6799e-02,  8.8781e-03,  1.7972e-02,\n",
       "                       2.7054e-02, -2.4239e-02,  1.3210e-02,  6.6333e-03, -7.8836e-03,\n",
       "                      -2.2811e-02,  9.1114e-03,  1.2452e-02, -2.6346e-02, -2.2719e-02,\n",
       "                      -2.9904e-02,  1.2384e-02,  1.7807e-02, -3.5314e-02, -1.0156e-02,\n",
       "                      -6.4484e-03, -1.7128e-02, -2.0069e-02,  3.0776e-02, -1.3223e-02,\n",
       "                       1.4159e-02,  2.2510e-03,  3.1007e-02,  2.3535e-02,  2.4056e-02,\n",
       "                       2.8225e-02, -2.8796e-02,  1.4001e-02, -5.1249e-03, -6.9402e-03,\n",
       "                       1.3761e-02,  2.3081e-02, -7.9799e-03, -1.2827e-02, -2.3250e-02,\n",
       "                      -1.9010e-02,  2.0731e-02,  1.2212e-02, -1.3956e-04,  8.6249e-03,\n",
       "                       3.5072e-02, -2.3994e-02,  2.6850e-02, -2.8261e-02, -3.2036e-02,\n",
       "                      -3.4193e-02,  3.0856e-02, -9.2357e-03,  2.0022e-02, -4.2818e-03,\n",
       "                      -1.5700e-04,  2.7809e-02,  1.8267e-02,  8.2796e-03, -1.7629e-02,\n",
       "                       3.4339e-02,  3.2628e-03, -2.5526e-02, -1.9025e-02,  3.8497e-03,\n",
       "                       1.7478e-02,  4.4111e-02, -1.7763e-02,  2.1327e-02, -2.8245e-02,\n",
       "                       1.7807e-02,  3.1528e-02,  3.4168e-02,  2.7252e-03,  1.5297e-02,\n",
       "                       2.3016e-02,  2.3546e-02,  2.5178e-02, -2.2945e-02,  2.4179e-02,\n",
       "                       3.4174e-02, -2.4145e-02,  1.9245e-02, -2.0205e-02,  3.8177e-02,\n",
       "                       1.6545e-02, -2.9276e-02, -4.1056e-03, -3.2303e-03, -4.7207e-03,\n",
       "                       2.9083e-02, -2.2009e-02,  3.6102e-02,  2.4402e-02,  8.9949e-03,\n",
       "                       1.1681e-02, -3.1674e-02, -1.7857e-02,  4.3382e-03,  3.1879e-02,\n",
       "                       2.1919e-02, -1.2729e-02, -7.6099e-03,  2.4714e-02,  2.8966e-02,\n",
       "                      -1.0192e-02,  2.6738e-02, -2.7064e-02,  2.1653e-02,  1.1734e-02,\n",
       "                       2.8906e-02,  3.7647e-02,  9.3644e-03,  2.1950e-02,  3.5884e-02,\n",
       "                       9.7324e-03, -1.2892e-02, -1.5656e-03,  3.4956e-02, -3.0608e-03,\n",
       "                      -3.7084e-02, -2.1738e-02,  2.5724e-02,  4.0386e-03,  2.8381e-03,\n",
       "                       1.5200e-02,  2.4703e-02,  2.2756e-02,  3.2320e-02,  2.8638e-02,\n",
       "                      -3.6737e-03,  2.2628e-02, -2.5118e-02,  3.2846e-02,  3.1050e-02,\n",
       "                      -2.2012e-02,  3.5624e-02,  7.6651e-03, -7.7967e-03,  9.7992e-03,\n",
       "                      -7.6667e-03,  1.9950e-02,  2.1137e-03, -6.7118e-03,  9.2121e-03,\n",
       "                       3.5507e-02,  2.5119e-02,  3.1106e-02, -1.4616e-02,  2.9006e-02,\n",
       "                       1.9031e-02,  2.3115e-02,  2.4042e-02, -3.2134e-02, -2.8153e-02,\n",
       "                      -1.0337e-02, -1.9356e-02,  1.9402e-03,  1.3082e-02,  8.9455e-03,\n",
       "                      -2.1169e-02, -3.1159e-02, -6.6951e-03,  1.6853e-02, -2.8618e-02,\n",
       "                      -2.6670e-02,  1.4400e-03, -2.5769e-02, -2.2251e-03, -3.5841e-02,\n",
       "                      -1.4627e-02,  2.5487e-02, -3.4778e-03, -8.4367e-03,  4.2135e-02,\n",
       "                      -4.7793e-03,  6.6428e-03, -3.2628e-02, -5.3361e-03, -2.0344e-02,\n",
       "                      -7.6838e-03,  4.7515e-03,  3.4051e-02,  3.3070e-02, -2.7424e-02,\n",
       "                      -1.8139e-02,  3.2825e-02,  1.5196e-02, -6.9843e-03,  3.2427e-02,\n",
       "                       2.0766e-02,  4.5832e-03, -3.0228e-02, -2.5695e-02,  3.1303e-02,\n",
       "                       1.6272e-02, -8.4270e-03, -2.5744e-02,  1.0881e-02,  9.7586e-03,\n",
       "                       1.0064e-02,  2.4648e-02,  2.1276e-02,  2.7006e-02, -5.1540e-03,\n",
       "                       1.3829e-02,  2.1892e-02,  4.7795e-02, -1.6743e-02,  1.7510e-02,\n",
       "                       2.5238e-02, -2.8036e-02, -2.5256e-04, -1.2312e-02,  9.0388e-03,\n",
       "                       1.4993e-02,  6.7284e-03,  3.5316e-02, -3.5040e-02,  1.0204e-02,\n",
       "                       6.7096e-03, -1.7048e-02,  5.7233e-03,  2.0688e-02, -2.1532e-02,\n",
       "                       1.4242e-02,  3.2711e-02,  4.1746e-03,  1.0765e-02, -2.5168e-02,\n",
       "                       1.6888e-02,  2.7899e-02,  1.7513e-02,  3.3111e-02, -2.2372e-02,\n",
       "                       1.2278e-02, -1.7982e-02])),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[-0.0162,  0.0212,  0.0140,  ...,  0.0081, -0.0044,  0.0309],\n",
       "                      [ 0.0488,  0.0015,  0.0412,  ..., -0.0406, -0.0323,  0.0052],\n",
       "                      [-0.0463,  0.0388,  0.0367,  ...,  0.0187, -0.0004,  0.0275],\n",
       "                      ...,\n",
       "                      [ 0.0282, -0.0231, -0.0344,  ...,  0.0225, -0.0170, -0.0089],\n",
       "                      [ 0.0030, -0.0130,  0.0417,  ..., -0.0291, -0.0277,  0.0307],\n",
       "                      [ 0.0283, -0.0310, -0.0356,  ...,  0.0420,  0.0074,  0.0062]])),\n",
       "             ('linear_relu_stack.2.bias',\n",
       "              tensor([-3.0148e-02, -3.3248e-02,  4.4339e-02, -3.0288e-03,  3.6391e-02,\n",
       "                       3.7131e-02,  7.7567e-03, -3.9148e-02, -1.9067e-02, -1.3687e-02,\n",
       "                       1.9924e-02,  6.7426e-03, -2.6152e-02, -3.8886e-02, -2.7786e-02,\n",
       "                      -1.3300e-02,  4.0303e-03,  2.3146e-02, -2.6548e-03,  4.1548e-02,\n",
       "                      -3.3383e-02,  2.9041e-02, -1.7051e-02, -3.4818e-02,  3.1738e-02,\n",
       "                      -4.0201e-02,  1.1143e-02,  4.1629e-02, -3.5513e-02, -4.2046e-02,\n",
       "                      -2.8418e-03, -1.1965e-03,  5.1565e-02, -2.6040e-02,  3.5627e-02,\n",
       "                      -1.0426e-02, -5.7815e-03,  2.5117e-02, -3.9530e-02, -1.3628e-02,\n",
       "                      -4.3702e-02, -2.0353e-02, -1.3715e-02, -9.0861e-04, -3.1036e-02,\n",
       "                       5.6008e-03, -3.1262e-02,  5.7722e-03,  2.2208e-02, -2.1459e-02,\n",
       "                      -3.1535e-02, -3.0025e-02, -2.8090e-02,  6.6077e-02, -2.5397e-02,\n",
       "                       2.7643e-02,  1.6861e-02,  3.9132e-02, -2.4049e-02, -4.5188e-02,\n",
       "                       4.7381e-02, -2.9432e-02, -1.3049e-02,  2.0484e-03,  2.0868e-02,\n",
       "                       3.0359e-02, -3.4449e-02, -1.2456e-02,  2.6099e-02,  7.1021e-03,\n",
       "                      -3.2081e-02, -2.9798e-03,  3.9883e-02,  1.8474e-02, -2.5046e-02,\n",
       "                       2.8066e-02, -1.5480e-02,  7.4139e-03, -3.9230e-02,  2.3700e-02,\n",
       "                      -3.1244e-02, -1.0935e-03, -3.7970e-02,  2.4792e-02, -3.6368e-02,\n",
       "                      -2.2049e-02, -2.8825e-02,  6.8881e-03, -4.4052e-03, -3.8381e-02,\n",
       "                       1.4008e-03,  3.8239e-02,  2.6785e-02, -4.4917e-02,  5.7353e-02,\n",
       "                       3.9812e-02,  4.9905e-03,  2.9291e-02,  3.3551e-02,  3.5922e-02,\n",
       "                      -3.0271e-02,  1.8068e-02,  3.6416e-02, -2.3030e-02, -1.9829e-02,\n",
       "                      -7.7490e-03, -2.1870e-02,  4.9457e-02, -5.4142e-03, -2.8347e-03,\n",
       "                      -3.0514e-02, -3.5490e-02,  3.5790e-04, -1.9917e-02, -4.3305e-02,\n",
       "                       3.3104e-03, -2.2961e-02,  3.2219e-03,  1.5665e-03, -1.7781e-03,\n",
       "                      -1.2594e-02,  1.0790e-02,  2.6861e-02, -3.5030e-02, -1.6683e-02,\n",
       "                      -1.9396e-02, -1.7180e-02,  3.6190e-02, -3.0827e-02, -2.4443e-02,\n",
       "                      -3.1727e-03,  8.3395e-03, -2.1171e-02, -8.6142e-03, -4.3904e-02,\n",
       "                      -4.2407e-02,  2.4097e-02, -1.4864e-02,  1.8956e-02, -2.4586e-03,\n",
       "                      -3.3574e-02,  4.4225e-02,  4.0861e-02, -1.9571e-02, -7.4214e-03,\n",
       "                       1.8662e-02,  4.2531e-02,  3.0786e-02, -1.8263e-02,  3.3154e-05,\n",
       "                      -7.2728e-03,  2.7486e-02, -1.3837e-02,  3.7537e-02, -1.2881e-02,\n",
       "                      -3.9179e-02,  3.8344e-02,  2.4804e-02, -2.1571e-02,  2.5111e-02,\n",
       "                       1.6848e-02,  3.5529e-02, -1.8549e-02, -1.7563e-02, -3.8806e-02,\n",
       "                       3.6598e-02,  1.6377e-02,  5.7491e-03,  3.3596e-03,  2.2394e-02,\n",
       "                      -3.4298e-02,  1.5716e-02,  2.5847e-02,  5.2946e-02,  4.3151e-03,\n",
       "                       3.3056e-02,  1.5028e-02, -1.4335e-02,  1.9691e-02,  2.5290e-03,\n",
       "                      -3.2297e-02, -2.0828e-02, -3.5883e-03,  3.6243e-02,  3.6335e-02,\n",
       "                       3.2378e-02,  9.9661e-03, -8.6367e-03, -1.7263e-03, -1.4162e-02,\n",
       "                       1.2221e-02,  1.7842e-03, -4.3383e-02, -1.4594e-02,  3.9194e-02,\n",
       "                      -2.6515e-02,  2.6747e-02, -4.9644e-02, -4.0031e-02, -2.6334e-02,\n",
       "                       1.6101e-03, -1.4260e-02,  3.6568e-02, -1.3304e-02, -1.8078e-02,\n",
       "                       3.3294e-02,  1.2870e-03,  1.3375e-02, -9.3850e-04, -3.8584e-02,\n",
       "                       1.5191e-02,  5.9810e-03, -4.2661e-02,  5.1510e-02,  3.1962e-02,\n",
       "                       1.9605e-02,  2.7347e-03,  3.2735e-02, -3.6178e-03, -7.1417e-03,\n",
       "                       1.4810e-03,  7.9832e-03,  2.5830e-02, -3.2724e-02,  2.4179e-02,\n",
       "                      -1.6347e-02,  1.2803e-02,  2.0614e-02,  1.3567e-02, -3.2884e-02,\n",
       "                       2.8057e-02, -6.5063e-03,  2.8326e-02,  1.2906e-02,  7.9986e-03,\n",
       "                      -2.9473e-02, -9.7147e-03,  2.7164e-02,  8.5321e-03,  4.5251e-02,\n",
       "                      -4.4082e-02,  2.1633e-03,  3.0823e-02,  1.8755e-02, -3.2305e-03,\n",
       "                      -2.6852e-02,  9.4066e-03,  4.5524e-03,  1.9163e-02, -5.3629e-03,\n",
       "                      -4.1569e-02, -1.5191e-02,  4.0176e-02, -3.3674e-02, -3.4619e-02,\n",
       "                      -1.5046e-02, -5.6450e-03, -3.2855e-02,  3.2191e-03,  4.7012e-03,\n",
       "                      -1.6241e-02, -4.5659e-02,  3.3026e-02, -6.3884e-03,  2.9395e-02,\n",
       "                      -2.4791e-03,  2.9499e-02,  1.2541e-02,  3.6039e-02,  4.6126e-02,\n",
       "                       5.1077e-02,  3.3841e-03,  3.7619e-02, -2.1947e-03,  1.7900e-02,\n",
       "                       2.2246e-02,  2.7009e-02, -1.2236e-03,  9.5231e-03, -1.1612e-02,\n",
       "                      -4.2561e-02, -1.9255e-02, -2.1996e-02, -2.3818e-02,  1.3420e-02,\n",
       "                       5.7434e-03, -3.2822e-02,  4.1025e-02, -4.1911e-02, -4.2022e-02,\n",
       "                      -6.9899e-03, -4.0797e-02,  1.1740e-02, -2.8375e-02, -1.8775e-02,\n",
       "                       1.0858e-02,  2.0961e-02,  3.8232e-02,  1.7042e-02,  3.1293e-02,\n",
       "                      -3.4344e-02, -8.6918e-03, -1.9785e-02,  2.6739e-02,  1.8036e-02,\n",
       "                      -1.0731e-02,  1.4270e-03,  4.0384e-02,  2.2588e-02, -2.7569e-03,\n",
       "                       3.7938e-04, -1.9682e-02, -2.4144e-02, -2.4188e-02, -3.1931e-02,\n",
       "                      -3.2420e-03, -2.7135e-02, -4.1492e-03, -7.0267e-03, -1.3768e-02,\n",
       "                       2.0654e-02, -2.1013e-02,  1.6032e-02, -3.9115e-02, -2.9274e-02,\n",
       "                       3.8413e-02, -2.3660e-02, -2.6787e-02,  3.5364e-03,  9.9834e-03,\n",
       "                      -1.3118e-02, -2.6781e-02, -3.8873e-02,  4.2119e-02,  5.3848e-03,\n",
       "                       6.7255e-02,  4.9056e-03,  2.3375e-02, -1.5366e-02, -6.1479e-03,\n",
       "                      -4.1224e-03,  4.0249e-02,  4.7206e-03,  1.7611e-02,  3.7551e-02,\n",
       "                       2.1295e-03, -1.2810e-02, -9.3550e-03,  1.7791e-02, -1.4350e-02,\n",
       "                       2.4021e-02, -2.4851e-03, -3.2508e-02,  7.5950e-03, -2.6551e-02,\n",
       "                      -7.7095e-04, -1.0972e-02, -4.3520e-02,  2.2689e-02,  2.0314e-02,\n",
       "                       1.3479e-02,  1.6083e-02,  1.6813e-02,  2.5615e-02,  6.7543e-03,\n",
       "                      -1.4664e-02,  3.3540e-02, -1.1557e-02, -1.5184e-02,  2.0452e-02,\n",
       "                      -1.9462e-02,  4.1089e-02, -3.7035e-02, -4.3240e-02, -1.4588e-02,\n",
       "                      -1.5634e-02,  1.0118e-02,  1.1396e-02,  2.4109e-02, -2.5482e-02,\n",
       "                      -3.5455e-02, -2.6628e-02,  7.8837e-04, -3.0347e-02,  3.7840e-02,\n",
       "                      -7.7606e-03, -2.1684e-02,  2.7609e-02, -1.5092e-02,  2.4361e-02,\n",
       "                       4.4937e-02, -2.3603e-02,  3.7966e-02, -3.5481e-02, -1.9143e-03,\n",
       "                       1.8119e-02, -3.4712e-02,  2.8012e-02, -2.5048e-02, -3.5034e-02,\n",
       "                       3.2293e-02, -2.8406e-02,  3.2587e-02,  3.0342e-02, -3.4497e-02,\n",
       "                       1.2175e-02,  3.5443e-02, -7.4752e-03,  6.1296e-02, -3.9387e-02,\n",
       "                       4.9390e-02,  2.8958e-02, -4.4675e-03, -3.4701e-02,  4.8166e-03,\n",
       "                       5.6236e-02,  1.6668e-03, -2.4491e-03,  3.2219e-02,  3.5338e-02,\n",
       "                       2.6234e-02, -2.8303e-02,  9.8950e-03,  4.1183e-03,  6.1688e-03,\n",
       "                       4.1407e-03,  1.5852e-02, -1.3991e-02, -3.6238e-02,  3.4990e-02,\n",
       "                       2.5022e-02,  1.0893e-03,  3.9832e-02, -2.7946e-02, -1.4207e-02,\n",
       "                      -1.1002e-02,  2.8351e-02, -3.1521e-02, -1.1279e-02,  2.4195e-02,\n",
       "                      -4.3733e-02, -2.3974e-02,  2.9337e-02, -2.7242e-02, -1.0897e-02,\n",
       "                      -3.0486e-02,  5.3357e-02, -2.7737e-02, -1.2707e-02, -1.1735e-02,\n",
       "                       4.2126e-03,  3.7183e-02, -8.7918e-03, -2.4178e-02, -4.5039e-02,\n",
       "                      -3.3644e-02, -2.7980e-02,  3.4631e-02,  4.7996e-02, -5.5419e-04,\n",
       "                      -1.4003e-02,  3.3972e-02, -1.6111e-02,  6.3932e-03, -3.6164e-02,\n",
       "                       4.5284e-02,  3.5714e-02,  6.2036e-03,  1.1300e-02,  1.5484e-02,\n",
       "                       1.3404e-02,  8.9048e-03, -1.8897e-02,  4.1785e-02,  2.1770e-02,\n",
       "                       6.9923e-03,  2.3661e-02,  3.9591e-02,  3.6821e-02, -7.9420e-03,\n",
       "                       3.6092e-02,  3.8267e-02,  2.1475e-02, -4.3027e-03, -5.9234e-03,\n",
       "                      -2.7068e-03,  2.7942e-02, -1.3025e-03, -2.0075e-02,  2.8585e-02,\n",
       "                       6.0114e-03,  1.2867e-02, -3.6157e-02,  1.5801e-02,  1.6425e-02,\n",
       "                       8.0543e-03, -2.7046e-02, -2.7939e-02, -7.9338e-03, -7.7658e-03,\n",
       "                       3.3162e-03, -8.4050e-03,  1.2848e-02, -1.4380e-02,  4.1963e-02,\n",
       "                       3.5151e-02,  1.1642e-02, -1.8725e-02,  1.9491e-02,  8.2378e-03,\n",
       "                      -3.3217e-02, -3.4772e-02])),\n",
       "             ('linear_relu_stack.4.weight',\n",
       "              tensor([[-0.0200,  0.0038, -0.0365,  ...,  0.0255, -0.0111,  0.0304],\n",
       "                      [ 0.1157, -0.0692, -0.0810,  ..., -0.0067, -0.0706,  0.0066],\n",
       "                      [-0.0308,  0.0406, -0.0445,  ..., -0.0115, -0.0622, -0.0384],\n",
       "                      ...,\n",
       "                      [-0.0637, -0.0793,  0.0835,  ...,  0.0184,  0.0807,  0.0066],\n",
       "                      [-0.0508,  0.0205,  0.0247,  ...,  0.0213,  0.0277,  0.0079],\n",
       "                      [-0.0061,  0.0591,  0.0417,  ..., -0.0190,  0.0617,  0.0068]])),\n",
       "             ('linear_relu_stack.4.bias',\n",
       "              tensor([-0.0500, -0.0213, -0.0215,  0.0237, -0.1187,  0.1439, -0.0444,  0.0509,\n",
       "                      -0.0544, -0.0280]))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_probs = []\n",
    "class_label = []\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        output = model(images)\n",
    "        class_probs_batch = [nn.functional.softmax(el, dim=0) for el in output]\n",
    "\n",
    "        class_probs.append(class_probs_batch)\n",
    "        class_label.append(labels)\n",
    "\n",
    "test_probs = torch.cat([torch.stack(batch) for batch in class_probs])\n",
    "test_label = torch.cat(class_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0037, 0.0244, 0.0454,  ..., 0.2419, 0.0690, 0.0141])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_probs[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0037, 0.0017, 0.0125, 0.0026, 0.0117, 0.2161, 0.0096, 0.2504, 0.1103,\n",
       "        0.3815])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "torch.cat([tensor, tensor], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([tensor, tensor], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int16)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(10, dtype=torch.int16).scatter_(dim=0, index=torch.tensor(1), src=torch.tensor(2, dtype=torch.int16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5],\n",
       "        [ 6,  7,  8,  9, 10]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = torch.arange(1, 11).reshape(2, 5)\n",
    "print(src.shape)\n",
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 0]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = torch.tensor([[0, 1, 2, 0]])\n",
    "print(index.shape)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self = torch.zeros(3, 5)\n",
    "print(self.shape)\n",
    "self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf],\n",
       "        [0., 0., -inf],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sz = 3\n",
    "torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d8025d30852334e6e768ca567da121c1aa274c2c5a5a8a9ff400eded44c1a99b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('transformers')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

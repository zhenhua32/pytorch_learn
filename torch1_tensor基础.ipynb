{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitbasecondae8929445447b42c891dc62f0523a76ee",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch 的 tensor 介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. 不修改自身数据, 返回一个新的 Tensor, 如 x.add(y)\n",
    "2. 修改自身数据, 如 x.add_(y), 运行符带下划线后缀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([4, 6])\ntensor([1, 2])\ntensor([4, 6])\n"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "z = x.add(y)  # 不修改 x 的数据\n",
    "print(z)\n",
    "print(x)\n",
    "x.add_(y)  # 就地版本, 更新 x 的数据\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "创建 Tensor\n",
    "![](https://res.weread.qq.com/wrepub/epub_26858491_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([1., 2., 3., 4.])\ntensor([[6.6280e-10, 4.1261e-08, 1.0386e+21],\n        [2.0314e+20, 4.3439e-05, 6.4536e-10]])\ntensor([[1., 2., 3.],\n        [4., 5., 6.]])\ntorch.Size([2, 3])\ntorch.Size([2, 3])\ntensor([[0.0000e+00, 0.0000e+00, 8.4078e-45],\n        [0.0000e+00, 1.4013e-45, 0.0000e+00]])\n"
    }
   ],
   "source": [
    "print(torch.Tensor([1, 2, 3, 4]))\n",
    "print(torch.Tensor(2, 3))\n",
    "t = torch.Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(t)\n",
    "print(t.size())\n",
    "print(t.shape)\n",
    "print(torch.Tensor(t.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "修改形状\n",
    "![修改形状](https://res.weread.qq.com/wrepub/epub_26858491_23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[-0.0498,  0.7371, -0.3035],\n        [ 1.1849,  1.3938,  1.2716]])\ntorch.Size([2, 3])\n2\ntensor([[-0.0498,  0.7371],\n        [-0.3035,  1.1849],\n        [ 1.3938,  1.2716]])\ntensor([-0.0498,  0.7371, -0.3035,  1.1849,  1.3938,  1.2716])\ntorch.Size([6])\ntensor([[-0.0498,  0.7371, -0.3035,  1.1849,  1.3938,  1.2716]])\ntorch.Size([1, 6])\n6\n"
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "print(x)\n",
    "print(x.size())\n",
    "print(x.dim())\n",
    "print(x.view(3,2))\n",
    "y = x.view(-1)\n",
    "print(y)\n",
    "print(y.shape)\n",
    "z = torch.unsqueeze(y, 0)  # 在第一维度上增加一个 1\n",
    "print(z)\n",
    "print(z.shape)\n",
    "print(z.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "选择操作\n",
    "![选择操作](https://res.weread.qq.com/wrepub/epub_26858491_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[ 0.3607, -0.2859, -0.3938],\n        [ 0.2429, -1.3833, -2.3134]])\ntensor([ 0.3607, -0.2859, -0.3938])\ntensor([-0.3938, -2.3134])\ntensor([[ True, False, False],\n        [ True, False, False]])\ntensor([0.3607, 0.2429])\ntensor([[0, 0],\n        [1, 0]])\ntensor([[0, 1, 1]])\ntensor([[ 0.3607, -1.3833, -2.3134]])\ntensor([[ 0.3607, -0.2859, -0.2859],\n        [-1.3833, -1.3833, -1.3833]])\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\ntensor([[ 0.3607, -0.2859,  0.0000],\n        [ 0.0000, -1.3833,  0.0000]])\n"
    }
   ],
   "source": [
    "torch.manual_seed(100)  # 设置固定的随机种子\n",
    "x = torch.randn(2, 3)\n",
    "print(x)\n",
    "print(x[0, :])  # 第一行的所有数据\n",
    "print(x[:, -1])  # 最后一列的所有数据\n",
    "mask = x > 0  # 生成 bool 张量\n",
    "print(mask)\n",
    "y = torch.masked_select(x, mask)\n",
    "print(y)\n",
    "print(torch.nonzero(mask))  # 获取非零下标, 即行列索引\n",
    "# 获取指定索引对应的值, 输出根据以下规则得到\n",
    "# out[i][j] = input[index[i][j]][j]  # if dim == 0\n",
    "# out[i][j] = input[i][index[i][j]]  # if dim == 1\n",
    "index = torch.LongTensor([[0, 1, 1]])\n",
    "print(index)\n",
    "print(torch.gather(x, 0, index))  # dim == 0\n",
    "index = torch.LongTensor([[0, 1, 1], [1, 1, 1]])\n",
    "a = torch.gather(x , 1, index)  # dim == 1\n",
    "print(a)\n",
    "z = torch.zeros(2, 3)\n",
    "print(z)\n",
    "b = torch.scatter(z, 1, index, a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[ 0],\n        [10],\n        [20],\n        [30]], dtype=torch.int32)\ntensor([0, 1, 2], dtype=torch.int32)\ntensor([[ 0,  1,  2],\n        [10, 11, 12],\n        [20, 21, 22],\n        [30, 31, 32]], dtype=torch.int32)\ntensor([[ 0,  0,  0],\n        [10, 10, 10],\n        [20, 20, 20],\n        [30, 30, 30]], dtype=torch.int32)\ntensor([[0, 1, 2],\n        [0, 1, 2],\n        [0, 1, 2],\n        [0, 1, 2]], dtype=torch.int32)\ntensor([[ 0,  1,  2],\n        [10, 11, 12],\n        [20, 21, 22],\n        [30, 31, 32]], dtype=torch.int32)\n"
    }
   ],
   "source": [
    "# torch 的广播机制示例\n",
    "import numpy as np \n",
    "A = np.arange(0, 40, 10).reshape(4, 1)\n",
    "B = np.arange(0, 3)\n",
    "A1 = torch.from_numpy(A)\n",
    "B1 = torch.from_numpy(B)\n",
    "print(A1)\n",
    "print(B1)\n",
    "C = A1 + B1\n",
    "print(C)\n",
    "\n",
    "# 手动配置, 结果为每个轴上最大的, 所以结果的形状为 (4, 3)\n",
    "B2 = B1.unsqueeze(0)  # B2 的形状为 (1, 3)\n",
    "A2 = A1.expand(4, 3)  # 重复数组, 变成 (4, 3)\n",
    "B3 = B2.expand(4, 3)  # 重复数组, 变成 (4, 3)\n",
    "print(A2)\n",
    "print(B3)\n",
    "C1 = A2 + B3\n",
    "print(C1)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "逐元素操作\n",
    "![逐元素操作](https://res.weread.qq.com/wrepub/epub_26858491_25)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "归并操作\n",
    "![归并操作](https://res.weread.qq.com/wrepub/epub_26858491_26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[ 0.,  2.,  4.],\n        [ 6.,  8., 10.]])\ntensor([ 6., 10., 14.])\ntensor([[ 6., 10., 14.]])\n"
    }
   ],
   "source": [
    "a = torch.linspace(0, 10, 6)\n",
    "a = a.view((2, 3))\n",
    "print(a)\n",
    "b = a.sum(dim=0)\n",
    "print(b)\n",
    "b = a.sum(dim=0, keepdim=True)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "比较函数\n",
    "![比较函数](https://res.weread.qq.com/wrepub/epub_26858491_27)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "矩阵操作\n",
    "![矩阵操作](https://res.weread.qq.com/wrepub/epub_26858491_28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor(18)\ntensor([[ 6,  8,  4,  0],\n        [26, 43, 24,  0]])\ntensor([[[2, 5, 3],\n         [6, 7, 8]],\n\n        [[1, 2, 3],\n         [2, 7, 9]]])\ntensor([[[3, 9, 7, 7],\n         [5, 9, 2, 2],\n         [4, 8, 9, 6]],\n\n        [[1, 4, 5, 6],\n         [0, 4, 5, 8],\n         [0, 8, 9, 6]]])\ntensor([[[ 43,  87,  51,  42],\n         [ 85, 181, 128, 104]],\n\n        [[  1,  36,  42,  40],\n         [  2, 108, 126, 122]]])\ntorch.Size([2, 2, 4])\n"
    }
   ],
   "source": [
    "a = torch.tensor([2, 3])\n",
    "b = torch.tensor([3, 4])\n",
    "print(torch.dot(a, b))\n",
    "x = torch.randint(10, (2, 3))\n",
    "y = torch.randint(6, (3, 4))\n",
    "print(torch.mm(x, y))\n",
    "x = torch.randint(10, (2, 2, 3))\n",
    "y = torch.randint(10, (2, 3, 4))\n",
    "print(x)\n",
    "print(y)\n",
    "print(torch.bmm(x, y))\n",
    "print(torch.bmm(x, y).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PyTorch 和 Numpy 函数对照表\n",
    "![](https://res.weread.qq.com/wrepub/epub_26858491_29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}